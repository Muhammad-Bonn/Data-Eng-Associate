Daheeh YouTube Data Pipeline (Airflow + Docker) — This project automates the YouTube data extraction pipeline for the Arabic educational show "الدحيح (Daheeh)" using Apache Airflow, Docker, and Python. It collects data from multiple YouTube playlists, enriches it with metadata (views, likes, duration, comments), and stores it automatically in a SQLite database — all inside a single Docker container. --- ## 🧩 Project Overview — Goal: Fetch, clean, and store YouTube video data from Daheeh playlists. — Tech Stack: Python 3, Google YouTube Data API v3, Apache Airflow (2.9.0), Docker + Docker Compose, SQLite. — The DAG runs automatically every Tuesday and Saturday at 9:30 PM, and once immediately after the container starts. --- ## 📁 Project Structure — daheeh/ → dags/ → daheeh_pipeline.py (Airflow DAG) → scripts/ → extract_id.py, extract_metadata.py, main.py (ETL scripts) → data/ → youtube_database.db (output) → requirements.txt → Dockerfile → docker-compose.yml → entrypoint.sh --- ## ⚙️ How It Works — 1️⃣ When the container starts, it creates a virtual environment, installs dependencies, and sets up Airflow. — 2️⃣ The user enters their YouTube API key. — 3️⃣ Airflow runs the DAG: extract_id.py → extract_metadata.py → main.py → saves to SQLite. — 4️⃣ The resulting database file (youtube_database.db) is saved in /data locally. --- ## 🚀 How to Run — 1. Clone: git clone https://github.com/Muhammad-Bonn/doc-data-airflow.git && cd doc-data-airflow/daheeh — 2. Build & Run: docker-compose up --build — 3. Enter API Key: (from Google Cloud Console) — 4. Access Airflow: Username airflow, Password airflow → view DAG daheeh_youtube_pipeline. — 5. Check Results: sqlite3 data/youtube_database.db. --- ## 🔮 Future Improvements — Add cleaning and transformation steps, upload data to PostgreSQL/BigQuery, build dashboards (Streamlit/Metabase), and monitor API quota via Airflow sensors. --- ## 👥 Authors — Muhammad Ben, Omar, Hossam, and Zain — 🛠️ Built with passion for data automation & open learning
